{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b791ba-5675-4eca-9adc-e91e3ef2abcb",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12ecb44-ad08-4bd1-abe7-1dae0acf05e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 25 02:53:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:00:06.0 Off |                  N/A |\n",
      "| 27%   40C    P8    20W / 250W |   1404MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:00:0A.0 Off |                  N/A |\n",
      "| 27%   40C    P8    22W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     20729      C   ...nnh8/torch_env/bin/python     1401MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512559bc-07f2-424d-a77b-437cb4c49779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data/tungtx2/tmp/transformers_hub'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e10e97-9052-42f2-923e-d990d726e392",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tungtx2/env_ocr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac1f19-1c52-481f-8bc6-b33a277d4189",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227e3d1e-07b9-4309-a38f-1a3803f17b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "def normalize_bbox(bbox, width, height):\n",
    "     return [\n",
    "         int(1000 * (bbox[0] / width)),\n",
    "         int(1000 * (bbox[1] / height)),\n",
    "         int(1000 * (bbox[2] / width)),\n",
    "         int(1000 * (bbox[3] / height)),\n",
    "     ]\n",
    "\n",
    "def gen_annotations(dir, max_sample=1e9, block_type=None):\n",
    "  ls_words, ls_boxes, ls_labels, ls_img_fp = [], [], [], []\n",
    "  cnt = 0\n",
    "  for json_fp in Path(dir).rglob('*.json'):\n",
    "    if block_type != None and block_type not in str(json_fp):\n",
    "      continue\n",
    "    img_fp = json_fp.with_suffix('.jpg')\n",
    "\n",
    "    words, boxes, labels = [], [], []\n",
    "    with open(json_fp) as f:\n",
    "      json_data = json.load(f)\n",
    "    \n",
    "    img_h, img_w = json_data['imageHeight'], json_data['imageWidth']\n",
    "    for i, shape in enumerate(json_data['shapes']):\n",
    "      words.append(unidecode.unidecode(shape['text'].lower()))\n",
    "      # words.append(shape['text'].lower())\n",
    "      labels.append(shape['label'])\n",
    "      pts = [coord for pt in shape['points'] for coord in pt]\n",
    "      xmin = min(pts[0::2])\n",
    "      xmax = max(pts[0::2])\n",
    "      ymin = min(pts[1::2])\n",
    "      ymax = max(pts[1::2])\n",
    "\n",
    "      xmin = max(xmin, 0)\n",
    "      ymin = max(ymin, 0)\n",
    "      xmax = min(img_w, xmax)\n",
    "      ymax = min(img_h, ymax)\n",
    "\n",
    "      boxes.append(normalize_bbox((xmin, ymin, xmax, ymax), img_w, img_h))\n",
    "    ls_words.append(words)\n",
    "    ls_boxes.append(boxes)\n",
    "    ls_labels.append(labels)\n",
    "    ls_img_fp.append(str(img_fp))\n",
    "\n",
    "    cnt += 1\n",
    "    if cnt >= max_sample:\n",
    "      break\n",
    "\n",
    "  return ls_words, ls_boxes, ls_labels, ls_img_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92cf037-24f8-4aeb-bc14-be50f3bb0aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':kibggbe1hya', 'no', 'swift', 'overijssel', 'almelo', '7607dh', ':telshoek', 'add', '18', 'nigeria']\n",
      "[[390, 829, 496, 846], [364, 824, 385, 850], [329, 822, 364, 850], [587, 801, 676, 821], [521, 799, 583, 822], [466, 799, 516, 822], [360, 799, 444, 822], [330, 799, 356, 824], [447, 799, 463, 822], [807, 764, 871, 787]]\n",
      "['swift_code', 'marker_swift_code', 'marker_swift_code', 'bank_address', 'bank_address', 'bank_address', 'bank_address', 'marker_bank_address', 'bank_address', 'bank_name']\n",
      "fake_data_24032023/non_masked/train/fake_8124.jpg\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'fake_data_24032023/non_masked/train'\n",
    "val_dir = 'fake_data_24032023/non_masked/val'\n",
    "\n",
    "ls_words_train, ls_boxes_train, ls_labels_train, ls_img_train = gen_annotations(train_dir, max_sample=1e9)\n",
    "ls_words_val, ls_boxes_val, ls_labels_val, ls_img_val = gen_annotations(val_dir, max_sample=1e9)\n",
    "\n",
    "print(ls_words_train[0][:10])\n",
    "print(ls_boxes_train[0][:10])\n",
    "print(ls_labels_train[0][:10])\n",
    "print(ls_img_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482dc45d-08dd-471f-a185-af815c6aa83b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n",
      "111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ls_words_train))\n",
    "print(len(ls_words_val))\n",
    "\n",
    "len(ls_boxes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a381413e-0c43-4475-92b9-2914a7cdafc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'swift_code': 1380,\n",
       "         'marker_swift_code': 2841,\n",
       "         'bank_address': 7410,\n",
       "         'marker_bank_address': 865,\n",
       "         'bank_name': 8343,\n",
       "         'marker_bank_name': 2549,\n",
       "         'account_number': 2233,\n",
       "         'marker_account_number': 3389,\n",
       "         'marker_company_name': 4118,\n",
       "         'text': 119281,\n",
       "         'tax': 1504,\n",
       "         'marker_tax': 2196,\n",
       "         'phone': 2215,\n",
       "         'marker_phone': 2194,\n",
       "         'represented_name': 6528,\n",
       "         'marker_represented_name': 2029,\n",
       "         'marker_fax': 2162,\n",
       "         'fax': 2222,\n",
       "         'company_address': 10512,\n",
       "         'marker_company_address': 1264,\n",
       "         'company_name': 10798})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_annotations = [ls_words_train, ls_boxes_train, ls_labels_train, ls_img_train]\n",
    "val_annotations = [ls_words_val, ls_boxes_val, ls_labels_val, ls_img_val]\n",
    "# test = [ls_words_test, ls_boxes_test, ls_labels_test, ls_img_test]\n",
    "all_labels = [item for sublist in train_annotations[2] for item in sublist] + [item for sublist in val_annotations[2] for item in sublist]\n",
    "Counter(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ae76e4-d587-4a5a-970e-cd7c5803b103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marker_phone', 'company_address', 'represented_name', 'marker_tax', 'bank_name', 'tax', 'marker_swift_code', 'text', 'marker_bank_name', 'marker_bank_address', 'bank_address', 'marker_company_name', 'fax', 'swift_code', 'account_number', 'marker_represented_name', 'marker_company_address', 'marker_account_number', 'company_name', 'marker_fax', 'phone']\n",
      "{'marker_phone': 0, 'company_address': 1, 'represented_name': 2, 'marker_tax': 3, 'bank_name': 4, 'tax': 5, 'marker_swift_code': 6, 'text': 7, 'marker_bank_name': 8, 'marker_bank_address': 9, 'bank_address': 10, 'marker_company_name': 11, 'fax': 12, 'swift_code': 13, 'account_number': 14, 'marker_represented_name': 15, 'marker_company_address': 16, 'marker_account_number': 17, 'company_name': 18, 'marker_fax': 19, 'phone': 20}\n",
      "{0: 'marker_phone', 1: 'company_address', 2: 'represented_name', 3: 'marker_tax', 4: 'bank_name', 5: 'tax', 6: 'marker_swift_code', 7: 'text', 8: 'marker_bank_name', 9: 'marker_bank_address', 10: 'bank_address', 11: 'marker_company_name', 12: 'fax', 13: 'swift_code', 14: 'account_number', 15: 'marker_represented_name', 16: 'marker_company_address', 17: 'marker_account_number', 18: 'company_name', 19: 'marker_fax', 20: 'phone'}\n"
     ]
    }
   ],
   "source": [
    "labels = list(set(all_labels))\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "\n",
    "print(labels)\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "\n",
    "label_list = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a68f235-f682-43ad-a268-124b1cba628e",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4baace-baa1-435c-9a65-e915f144a5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class CORDDataset(Dataset):\n",
    "    \"\"\"CORD dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, annotations, processor=None, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations (List[List]): List of lists containing the word-level annotations (words, labels, boxes).\n",
    "            image_dir (string): Directory with all the document images.\n",
    "            processor (LayoutLMv2Processor): Processor to prepare the text + image.\n",
    "        \"\"\"\n",
    "        self.words, self.boxes, self.labels, self.img_paths = annotations\n",
    "        self.processor = processor\n",
    "        self.multi_split = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # first, take an image\n",
    "        item = self.img_paths[index]\n",
    "        image = Image.open(item).convert(\"RGB\")\n",
    "\n",
    "        # get word-level annotations \n",
    "        words = self.words[index]\n",
    "        boxes = self.boxes[index]\n",
    "        word_labels = self.labels[index]\n",
    "\n",
    "        word_labels = [label2id[label] for label in word_labels]\n",
    "\n",
    "        encoded_inputs = processor(image, words, boxes=boxes, word_labels=word_labels, truncation=True, stride =128, \n",
    "                            padding=\"max_length\", max_length=512, return_overflowing_tokens=True, return_offsets_mapping=True, return_tensors=\"pt\")  \n",
    "        \n",
    "        # print(encoded_inputs.keys())\n",
    "        overflow_to_sample_mapping = encoded_inputs.pop('overflow_to_sample_mapping')\n",
    "        offset_mapping = encoded_inputs.pop('offset_mapping')\n",
    "        # print('overflow_to_sample_mapping: ', overflow_to_sample_mapping)\n",
    "        # print('offset_mapping: ', offset_mapping)\n",
    "\n",
    "        # remove batch dimension\n",
    "        idx = np.random.randint(0, len(encoded_inputs['pixel_values']))\n",
    "        for k, v in encoded_inputs.items():\n",
    "            encoded_inputs[k] = v[idx]\n",
    "      \n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8ca1c4-2732-43ca-9539-b357eafb1513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutLMv3TokenizerFast(name_or_path='microsoft/layoutlmv3-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "False\n",
      "LayoutLMv3ImageProcessor {\n",
      "  \"apply_ocr\": false,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"LayoutLMv3FeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"LayoutLMv3ImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"ocr_lang\": null,\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"tesseract_config\": \"\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tungtx2/env_ocr/lib/python3.7/site-packages/transformers/models/layoutlmv3/processing_layoutlmv3.py:195: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3Processor, LayoutXLMTokenizerFast, LayoutLMv2FeatureExtractor\n",
    "\n",
    "# feature_extractor = LayoutLMv2FeatureExtractor(apply_ocr=False)\n",
    "# tokenizer = LayoutXLMTokenizerFast.from_pretrained('microsoft/layoutxlm-base')\n",
    "# tokenizer.only_label_first_subword = False\n",
    "# processor = LayoutXLMProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor = LayoutLMv3Processor.from_pretrained('microsoft/layoutlmv3-base', apply_ocr=False)\n",
    "processor.tokenizer.only_label_first_subword = False\n",
    "\n",
    "print(processor.tokenizer)\n",
    "print(processor.tokenizer.only_label_first_subword)\n",
    "print(processor.feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9043099-deab-4970-bc3c-346f2879aa34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CORDDataset(annotations=train_annotations,\n",
    "                            processor=processor)\n",
    "val_dataset = CORDDataset(annotations=val_annotations,\n",
    "                            processor=processor)\n",
    "# test_dataset = CORDDataset(annotations=test,\n",
    "#                             processor=processor)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e678182-0d92-450f-9c07-6aa4f5212bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "bbox torch.Size([512, 4])\n",
      "labels torch.Size([512])\n",
      "pixel_values torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "encoding= train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a6c7314-9dff-4fa9-9b8a-bd33ac09e081",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "(' :', 'swift_code', tensor([390, 829, 496, 846]))\n",
      "('k', 'swift_code', tensor([390, 829, 496, 846]))\n",
      "('ib', 'swift_code', tensor([390, 829, 496, 846]))\n",
      "('gg', 'swift_code', tensor([390, 829, 496, 846]))\n",
      "('be', 'swift_code', tensor([390, 829, 496, 846]))\n",
      "('1', 'swift_code', tensor([390, 829, 496, 846]))\n",
      "('hya', 'swift_code', tensor([390, 829, 496, 846]))\n",
      "(' no', 'marker_swift_code', tensor([364, 824, 385, 850]))\n",
      "(' swift', 'marker_swift_code', tensor([329, 822, 364, 850]))\n",
      "(' over', 'bank_address', tensor([587, 801, 676, 821]))\n",
      "('ij', 'bank_address', tensor([587, 801, 676, 821]))\n",
      "('s', 'bank_address', tensor([587, 801, 676, 821]))\n",
      "('sel', 'bank_address', tensor([587, 801, 676, 821]))\n",
      "(' al', 'bank_address', tensor([521, 799, 583, 822]))\n",
      "('mel', 'bank_address', tensor([521, 799, 583, 822]))\n",
      "('o', 'bank_address', tensor([521, 799, 583, 822]))\n",
      "(' 7', 'bank_address', tensor([466, 799, 516, 822]))\n",
      "('607', 'bank_address', tensor([466, 799, 516, 822]))\n",
      "('dh', 'bank_address', tensor([466, 799, 516, 822]))\n",
      "(' :', 'bank_address', tensor([360, 799, 444, 822]))\n",
      "('t', 'bank_address', tensor([360, 799, 444, 822]))\n",
      "('els', 'bank_address', tensor([360, 799, 444, 822]))\n",
      "('ho', 'bank_address', tensor([360, 799, 444, 822]))\n",
      "('ek', 'bank_address', tensor([360, 799, 444, 822]))\n",
      "(' add', 'marker_bank_address', tensor([330, 799, 356, 824]))\n",
      "(' 18', 'bank_address', tensor([447, 799, 463, 822]))\n",
      "(' nig', 'bank_name', tensor([807, 764, 871, 787]))\n",
      "('eria', 'bank_name', tensor([807, 764, 871, 787]))\n",
      "(' lag', 'bank_name', tensor([748, 764, 805, 787]))\n",
      "('os', 'bank_name', tensor([748, 764, 805, 787]))\n",
      "(',', 'bank_name', tensor([748, 764, 805, 787]))\n",
      "(' nig', 'bank_name', tensor([679, 764, 748, 787]))\n",
      "('eria', 'bank_name', tensor([679, 764, 748, 787]))\n",
      "(',', 'bank_name', tensor([679, 764, 748, 787]))\n",
      "(' name', 'marker_bank_name', tensor([513, 764, 561, 787]))\n",
      "(':', 'marker_bank_name', tensor([513, 764, 561, 787]))\n",
      "(' banker', 'marker_bank_name', tensor([436, 764, 509, 787]))\n",
      "(\"'s\", 'marker_bank_name', tensor([436, 764, 509, 787]))\n",
      "(' beneficiary', 'marker_bank_name', tensor([333, 764, 431, 787]))\n",
      "(' bank', 'bank_name', tensor([611, 764, 653, 787]))\n",
      "(' first', 'bank_name', tensor([564, 762, 610, 788]))\n",
      "(' of', 'bank_name', tensor([654, 762, 678, 788]))\n",
      "(' 7', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('-', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('173', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('-', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('51', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('(', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('us', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('d', 'account_number', tensor([454, 727, 553, 749]))\n",
      "(')', 'account_number', tensor([454, 727, 553, 749]))\n",
      "('3', 'account_number', tensor([454, 727, 553, 749]))\n",
      "(' number', 'marker_account_number', tensor([399, 727, 450, 749]))\n",
      "(':', 'marker_account_number', tensor([399, 727, 450, 749]))\n",
      "(' account', 'marker_account_number', tensor([333, 727, 381, 751]))\n",
      "(' beneficiary', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('s', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('name', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "(':', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('k', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('im', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('thank', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('c', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('..', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "('lt', 'marker_company_name', tensor([333, 696, 645, 715]))\n",
      "(' e', 'text', tensor([ 99, 660, 107, 668]))\n",
      "(' und', 'text', tensor([116, 657, 171, 669]))\n",
      "('ist', 'text', tensor([116, 657, 171, 669]))\n",
      "('ri', 'text', tensor([116, 657, 171, 669]))\n",
      "('h', 'text', tensor([116, 657, 171, 669]))\n",
      "('uted', 'text', tensor([116, 657, 171, 669]))\n",
      "(' (', 'text', tensor([769, 638, 835, 659]))\n",
      "('3', 'text', tensor([769, 638, 835, 659]))\n",
      "(',', 'text', tensor([769, 638, 835, 659]))\n",
      "('793', 'text', tensor([769, 638, 835, 659]))\n",
      "(',', 'text', tensor([769, 638, 835, 659]))\n",
      "('033', 'text', tensor([769, 638, 835, 659]))\n",
      "(',', 'text', tensor([769, 638, 835, 659]))\n",
      "('106', 'text', tensor([769, 638, 835, 659]))\n",
      "(')', 'text', tensor([769, 638, 835, 659]))\n",
      "(' (', 'text', tensor([332, 637, 398, 660]))\n",
      "('3', 'text', tensor([332, 637, 398, 660]))\n",
      "(',', 'text', tensor([332, 637, 398, 660]))\n",
      "('793', 'text', tensor([332, 637, 398, 660]))\n",
      "(',', 'text', tensor([332, 637, 398, 660]))\n",
      "('033', 'text', tensor([332, 637, 398, 660]))\n",
      "(',', 'text', tensor([332, 637, 398, 660]))\n",
      "('106', 'text', tensor([332, 637, 398, 660]))\n",
      "(')', 'text', tensor([332, 637, 398, 660]))\n",
      "(' (', 'text', tensor([246, 638, 311, 659]))\n",
      "('3', 'text', tensor([246, 638, 311, 659]))\n",
      "(',', 'text', tensor([246, 638, 311, 659]))\n",
      "('793', 'text', tensor([246, 638, 311, 659]))\n",
      "(',', 'text', tensor([246, 638, 311, 659]))\n",
      "('033', 'text', tensor([246, 638, 311, 659]))\n",
      "(',', 'text', tensor([246, 638, 311, 659]))\n",
      "('106', 'text', tensor([246, 638, 311, 659]))\n",
      "(')', 'text', tensor([246, 638, 311, 659]))\n",
      "(' value', 'text', tensor([141, 640, 166, 657]))\n",
      "(' fair', 'text', tensor([125, 640, 141, 659]))\n",
      "(' at', 'text', tensor([115, 641, 125, 657]))\n",
      "(' (', 'text', tensor([862, 637, 933, 659]))\n",
      "('3', 'text', tensor([862, 637, 933, 659]))\n",
      "(',', 'text', tensor([862, 637, 933, 659]))\n",
      "('793', 'text', tensor([862, 637, 933, 659]))\n",
      "(',', 'text', tensor([862, 637, 933, 659]))\n",
      "('033', 'text', tensor([862, 637, 933, 659]))\n",
      "(',', 'text', tensor([862, 637, 933, 659]))\n",
      "('106', 'text', tensor([862, 637, 933, 659]))\n",
      "(')', 'text', tensor([862, 637, 933, 659]))\n",
      "(' financial', 'text', tensor([115, 626, 151, 641]))\n",
      "(' assets', 'text', tensor([153, 624, 181, 641]))\n",
      "(' re', 'text', tensor([115, 610, 162, 626]))\n",
      "('val', 'text', tensor([115, 610, 162, 626]))\n",
      "('uation', 'text', tensor([115, 610, 162, 626]))\n",
      "(' of', 'text', tensor([163, 610, 175, 627]))\n",
      "(' from', 'text', tensor([114, 593, 137, 612]))\n",
      "(' differences', 'text', tensor([115, 579, 163, 594]))\n",
      "(' 4', 'text', tensor([101, 580, 107, 591]))\n",
      "(' reserve', 'text', tensor([115, 563, 148, 579]))\n",
      "(' 49', 'text', tensor([765, 560, 835, 580]))\n",
      "('2', 'text', tensor([765, 560, 835, 580]))\n",
      "(',', 'text', tensor([765, 560, 835, 580]))\n",
      "('9', 'text', tensor([765, 560, 835, 580]))\n",
      "('32', 'text', tensor([765, 560, 835, 580]))\n",
      "(',', 'text', tensor([765, 560, 835, 580]))\n",
      "('169', 'text', tensor([765, 560, 835, 580]))\n",
      "(',', 'text', tensor([765, 560, 835, 580]))\n",
      "('472', 'text', tensor([765, 560, 835, 580]))\n",
      "(' (', 'text', tensor([668, 562, 746, 579]))\n",
      "('492', 'text', tensor([668, 562, 746, 579]))\n",
      "(',', 'text', tensor([668, 562, 746, 579]))\n",
      "('9', 'text', tensor([668, 562, 746, 579]))\n",
      "('32', 'text', tensor([668, 562, 746, 579]))\n",
      "('.', 'text', tensor([668, 562, 746, 579]))\n",
      "('169', 'text', tensor([668, 562, 746, 579]))\n",
      "('.', 'text', tensor([668, 562, 746, 579]))\n",
      "('472', 'text', tensor([668, 562, 746, 579]))\n",
      "(')', 'text', tensor([668, 562, 746, 579]))\n",
      "(' 58', 'text', tensor([415, 562, 479, 579]))\n",
      "('.', 'text', tensor([415, 562, 479, 579]))\n",
      "('252', 'text', tensor([415, 562, 479, 579]))\n",
      "('.', 'text', tensor([415, 562, 479, 579]))\n",
      "('419', 'text', tensor([415, 562, 479, 579]))\n",
      "('.', 'text', tensor([415, 562, 479, 579]))\n",
      "('507', 'text', tensor([415, 562, 479, 579]))\n",
      "(' 49', 'text', tensor([329, 562, 397, 579]))\n",
      "('2', 'text', tensor([329, 562, 397, 579]))\n",
      "(',', 'text', tensor([329, 562, 397, 579]))\n",
      "('9', 'text', tensor([329, 562, 397, 579]))\n",
      "('32', 'text', tensor([329, 562, 397, 579]))\n",
      "(',', 'text', tensor([329, 562, 397, 579]))\n",
      "('169', 'text', tensor([329, 562, 397, 579]))\n",
      "(',', 'text', tensor([329, 562, 397, 579]))\n",
      "('472', 'text', tensor([329, 562, 397, 579]))\n",
      "(' 4', 'text', tensor([241, 562, 309, 579]))\n",
      "('34', 'text', tensor([241, 562, 309, 579]))\n",
      "(',', 'text', tensor([241, 562, 309, 579]))\n",
      "('679', 'text', tensor([241, 562, 309, 579]))\n",
      "(',', 'text', tensor([241, 562, 309, 579]))\n",
      "('7', 'text', tensor([241, 562, 309, 579]))\n",
      "('49', 'text', tensor([241, 562, 309, 579]))\n",
      "(',', 'text', tensor([241, 562, 309, 579]))\n",
      "('9', 'text', tensor([241, 562, 309, 579]))\n",
      "('65', 'text', tensor([241, 562, 309, 579]))\n",
      "(' financial', 'text', tensor([133, 547, 170, 563]))\n",
      "(' and', 'text', tensor([114, 546, 134, 565]))\n",
      "(' operational', 'text', tensor([115, 532, 164, 547]))\n",
      "(' th', 'text', tensor([ 97, 531, 110, 550]))\n",
      "('ien', 'text', tensor([ 97, 531, 110, 550]))\n",
      "(' risk', 'text', tensor([166, 528, 184, 549]))\n",
      "(' reserve', 'text', tensor([115, 517, 148, 532]))\n",
      "(' 58', 'text', tensor([863, 513, 932, 534]))\n",
      "(',', 'text', tensor([863, 513, 932, 534]))\n",
      "('252', 'text', tensor([863, 513, 932, 534]))\n",
      "(',', 'text', tensor([863, 513, 932, 534]))\n",
      "('419', 'text', tensor([863, 513, 932, 534]))\n",
      "(',', 'text', tensor([863, 513, 932, 534]))\n",
      "('507', 'text', tensor([863, 513, 932, 534]))\n",
      "(' 5', 'text', tensor([765, 515, 834, 532]))\n",
      "('77', 'text', tensor([765, 515, 834, 532]))\n",
      "('.', 'text', tensor([765, 515, 834, 532]))\n",
      "('439', 'text', tensor([765, 515, 834, 532]))\n",
      "('.', 'text', tensor([765, 515, 834, 532]))\n",
      "('7', 'text', tensor([765, 515, 834, 532]))\n",
      "('64', 'text', tensor([765, 515, 834, 532]))\n",
      "('.', 'text', tensor([765, 515, 834, 532]))\n",
      "('156', 'text', tensor([765, 515, 834, 532]))\n",
      "(' 58', 'text', tensor([416, 515, 479, 532]))\n",
      "('.', 'text', tensor([416, 515, 479, 532]))\n",
      "('252', 'text', tensor([416, 515, 479, 532]))\n",
      "('.', 'text', tensor([416, 515, 479, 532]))\n",
      "('419', 'text', tensor([416, 515, 479, 532]))\n",
      "('.', 'text', tensor([416, 515, 479, 532]))\n",
      "('507', 'text', tensor([416, 515, 479, 532]))\n",
      "(' 58', 'text', tensor([333, 513, 397, 534]))\n",
      "(',', 'text', tensor([333, 513, 397, 534]))\n",
      "('252', 'text', tensor([333, 513, 397, 534]))\n",
      "(',', 'text', tensor([333, 513, 397, 534]))\n",
      "('419', 'text', tensor([333, 513, 397, 534]))\n",
      "(',', 'text', tensor([333, 513, 397, 534]))\n",
      "('507', 'text', tensor([333, 513, 397, 534]))\n",
      "(' 5', 'text', tensor([241, 515, 309, 532]))\n",
      "('19', 'text', tensor([241, 515, 309, 532]))\n",
      "(',', 'text', tensor([241, 515, 309, 532]))\n",
      "('187', 'text', tensor([241, 515, 309, 532]))\n",
      "(',', 'text', tensor([241, 515, 309, 532]))\n",
      "('344', 'text', tensor([241, 515, 309, 532]))\n",
      "(',', 'text', tensor([241, 515, 309, 532]))\n",
      "('649', 'text', tensor([241, 515, 309, 532]))\n",
      "(' supplementary', 'text', tensor([115, 500, 178, 517]))\n",
      "(' capital', 'text', tensor([149, 483, 178, 500]))\n",
      "(' charter', 'text', tensor([114, 483, 146, 500]))\n",
      "('..', 'text', tensor([ 90, 475, 111, 503]))\n",
      "(' :', 'tax', tensor([279, 459, 355, 480]))\n",
      "('13', 'tax', tensor([279, 459, 355, 480]))\n",
      "('896', 'tax', tensor([279, 459, 355, 480]))\n",
      "('233', 'tax', tensor([279, 459, 355, 480]))\n",
      "('28', 'tax', tensor([279, 459, 355, 480]))\n",
      "(' code', 'marker_tax', tensor([236, 459, 279, 480]))\n",
      "(' tax', 'marker_tax', tensor([203, 456, 235, 482]))\n",
      "(' :', 'phone', tensor([234, 432, 308, 454]))\n",
      "('9', 'phone', tensor([234, 432, 308, 454]))\n",
      "('.', 'phone', tensor([234, 432, 308, 454]))\n",
      "('246', 'phone', tensor([234, 432, 308, 454]))\n",
      "('.', 'phone', tensor([234, 432, 308, 454]))\n",
      "('22', 'phone', tensor([234, 432, 308, 454]))\n",
      "('77', 'phone', tensor([234, 432, 308, 454]))\n",
      "(' tel', 'marker_phone', tensor([201, 429, 225, 455]))\n",
      "(' producer', 'represented_name', tensor([530, 401, 607, 423]))\n",
      "(' impact', 'represented_name', tensor([474, 401, 527, 423]))\n",
      "(' position', 'represented_name', tensor([400, 401, 470, 423]))\n",
      "(':', 'represented_name', tensor([400, 401, 470, 423]))\n",
      "(' m', 'represented_name', tensor([343, 401, 390, 423]))\n",
      "('oy', 'represented_name', tensor([343, 401, 390, 423]))\n",
      "('o', 'represented_name', tensor([343, 401, 390, 423]))\n",
      "(' representative', 'marker_represented_name', tensor([206, 401, 291, 423]))\n",
      "(' fax', 'marker_fax', tensor([641, 399, 669, 423]))\n",
      "(' l', 'represented_name', tensor([317, 399, 343, 424]))\n",
      "('iz', 'represented_name', tensor([317, 399, 343, 424]))\n",
      "(' by', 'marker_represented_name', tensor([294, 399, 317, 424]))\n",
      "(':', 'marker_represented_name', tensor([294, 399, 317, 424]))\n",
      "(' ::', 'fax', tensor([664, 397, 778, 424]))\n",
      "('(', 'fax', tensor([664, 397, 778, 424]))\n",
      "('+', 'fax', tensor([664, 397, 778, 424]))\n",
      "('0', 'fax', tensor([664, 397, 778, 424]))\n",
      "(')', 'fax', tensor([664, 397, 778, 424]))\n",
      "('65', 'fax', tensor([664, 397, 778, 424]))\n",
      "('82', 'fax', tensor([664, 397, 778, 424]))\n",
      "('.', 'fax', tensor([664, 397, 778, 424]))\n",
      "('9', 'fax', tensor([664, 397, 778, 424]))\n",
      "('.', 'fax', tensor([664, 397, 778, 424]))\n",
      "('57', 'fax', tensor([664, 397, 778, 424]))\n",
      "('27', 'fax', tensor([664, 397, 778, 424]))\n",
      "(' man', 'company_address', tensor([317, 371, 379, 393]))\n",
      "('aus', 'company_address', tensor([317, 371, 379, 393]))\n",
      "(' :', 'company_address', tensor([234, 371, 285, 393]))\n",
      "('u', 'company_address', tensor([234, 371, 285, 393]))\n",
      "('atro', 'company_address', tensor([234, 371, 285, 393]))\n",
      "(' 69', 'company_address', tensor([409, 369, 472, 393]))\n",
      "('043', 'company_address', tensor([409, 369, 472, 393]))\n",
      "('-', 'company_address', tensor([409, 369, 472, 393]))\n",
      "('150', 'company_address', tensor([409, 369, 472, 393]))\n",
      "(' am', 'company_address', tensor([382, 369, 407, 395]))\n",
      "(' 70', 'company_address', tensor([287, 371, 304, 393]))\n",
      "(' a', 'marker_company_address', tensor([203, 369, 231, 395]))\n",
      "('/', 'marker_company_address', tensor([203, 369, 231, 395]))\n",
      "('d', 'marker_company_address', tensor([203, 369, 231, 395]))\n",
      "(' enterprises', 'company_name', tensor([305, 339, 399, 360]))\n",
      "(' co', 'company_name', tensor([401, 335, 458, 362]))\n",
      "('.,', 'company_name', tensor([401, 335, 458, 362]))\n",
      "('lt', 'company_name', tensor([401, 335, 458, 362]))\n",
      "('d', 'company_name', tensor([401, 335, 458, 362]))\n",
      "(' du', 'company_name', tensor([248, 337, 302, 360]))\n",
      "('ong', 'company_name', tensor([248, 337, 302, 360]))\n",
      "(' d', 'company_name', tensor([202, 335, 247, 362]))\n",
      "('ong', 'company_name', tensor([202, 335, 247, 362]))\n",
      "(' lib', 'represented_name', tensor([398, 244, 540, 264]))\n",
      "('re', 'represented_name', tensor([398, 244, 540, 264]))\n",
      "('-', 'represented_name', tensor([398, 244, 540, 264]))\n",
      "('inter', 'represented_name', tensor([398, 244, 540, 264]))\n",
      "('active', 'represented_name', tensor([398, 244, 540, 264]))\n",
      "(' presented', 'marker_represented_name', tensor([200, 244, 256, 266]))\n",
      "(' fax', 'marker_fax', tensor([673, 243, 703, 268]))\n",
      "(':', 'marker_fax', tensor([673, 243, 703, 268]))\n",
      "(' manager', 'represented_name', tensor([545, 243, 617, 264]))\n",
      "(' cup', 'represented_name', tensor([335, 241, 397, 268]))\n",
      "('uan', 'represented_name', tensor([335, 241, 397, 268]))\n",
      "(' 0', 'fax', tensor([756, 240, 787, 270]))\n",
      "(')', 'fax', tensor([756, 240, 787, 270]))\n",
      "('38', 'fax', tensor([756, 240, 787, 270]))\n",
      "(' (', 'fax', tensor([734, 241, 757, 268]))\n",
      "('85', 'fax', tensor([734, 241, 757, 268]))\n",
      "(' 01', 'fax', tensor([714, 241, 734, 270]))\n",
      "(' miss', 'represented_name', tensor([297, 241, 333, 266]))\n",
      "(' :', 'text', tensor([285, 244, 294, 268]))\n",
      "(' by', 'marker_represented_name', tensor([258, 241, 279, 270]))\n",
      "(' +', 'fax', tensor([703, 241, 716, 270]))\n",
      "(' 8', 'tax', tensor([279, 216, 352, 238]))\n",
      "('306', 'tax', tensor([279, 216, 352, 238]))\n",
      "('38', 'tax', tensor([279, 216, 352, 238]))\n",
      "('26', 'tax', tensor([279, 216, 352, 238]))\n",
      "('24', 'tax', tensor([279, 216, 352, 238]))\n",
      "(' code', 'marker_tax', tensor([222, 213, 256, 241]))\n",
      "(' tax', 'marker_tax', tensor([197, 215, 222, 241]))\n",
      "(' :', 'text', tensor([261, 215, 271, 240]))\n",
      "(' :(', 'phone', tensor([243, 185, 324, 208]))\n",
      "('+', 'phone', tensor([243, 185, 324, 208]))\n",
      "('-', 'phone', tensor([243, 185, 324, 208]))\n",
      "('8', 'phone', tensor([243, 185, 324, 208]))\n",
      "('558', 'phone', tensor([243, 185, 324, 208]))\n",
      "('-)', 'phone', tensor([243, 185, 324, 208]))\n",
      "('159', 'phone', tensor([243, 185, 324, 208]))\n",
      "(' no', 'marker_phone', tensor([218, 182, 240, 212]))\n",
      "(' tel', 'marker_phone', tensor([197, 182, 219, 210]))\n",
      "(' b', 'company_address', tensor([507, 155, 530, 180]))\n",
      "('n', 'company_address', tensor([507, 155, 530, 180]))\n",
      "(' t', 'company_address', tensor([445, 155, 506, 179]))\n",
      "('ijuana', 'company_address', tensor([445, 155, 506, 179]))\n",
      "(' cp', 'company_address', tensor([388, 155, 442, 178]))\n",
      "('22', 'company_address', tensor([388, 155, 442, 178]))\n",
      "('100', 'company_address', tensor([388, 155, 442, 178]))\n",
      "(' se', 'company_address', tensor([296, 155, 384, 178]))\n",
      "('pt', 'company_address', tensor([296, 155, 384, 178]))\n",
      "('iem', 'company_address', tensor([296, 155, 384, 178]))\n",
      "('bre', 'company_address', tensor([296, 155, 384, 178]))\n",
      "(' address', 'marker_company_address', tensor([201, 155, 266, 178]))\n",
      "(' :', 'company_address', tensor([269, 154, 294, 180]))\n",
      "('de', 'company_address', tensor([269, 154, 294, 180]))\n",
      "('.', 'company_name', tensor([431, 126, 462, 150]))\n",
      "('lt', 'company_name', tensor([431, 126, 462, 150]))\n",
      "('d', 'company_name', tensor([431, 126, 462, 150]))\n",
      "(' co', 'company_name', tensor([403, 126, 435, 150]))\n",
      "('.,', 'company_name', tensor([403, 126, 435, 150]))\n",
      "('.', 'company_name', tensor([403, 126, 435, 150]))\n",
      "(' c', 'company_name', tensor([374, 126, 401, 150]))\n",
      "('ic', 'company_name', tensor([374, 126, 401, 150]))\n",
      "(' h', 'company_name', tensor([318, 126, 372, 149]))\n",
      "('u', 'company_name', tensor([318, 126, 372, 149]))\n",
      "('ong', 'company_name', tensor([318, 126, 372, 149]))\n",
      "(' :', 'company_name', tensor([274, 126, 316, 149]))\n",
      "('song', 'company_name', tensor([274, 126, 316, 149]))\n",
      "(' between', 'marker_company_name', tensor([199, 124, 270, 150]))\n",
      "(' equity', 'text', tensor([444, 103, 486, 120]))\n",
      "(' owners', 'text', tensor([383, 101, 439, 121]))\n",
      "(\"'\", 'text', tensor([383, 101, 439, 121]))\n",
      "(' changes', 'text', tensor([307, 101, 364, 120]))\n",
      "(' statement', 'text', tensor([212,  99, 283, 121]))\n",
      "(' separate', 'text', tensor([145,  99, 208, 121]))\n",
      "(' interim', 'text', tensor([ 95,  99, 143, 121]))\n",
      "(' in', 'text', tensor([367,  99, 382, 122]))\n",
      "(' of', 'text', tensor([284,  97, 305, 124]))\n",
      "(' b', 'text', tensor([846,  58, 911,  82]))\n",
      "('04', 'text', tensor([846,  58, 911,  82]))\n",
      "('a', 'text', tensor([846,  58, 911,  82]))\n",
      "('-', 'text', tensor([846,  58, 911,  82]))\n",
      "('ct', 'text', tensor([846,  58, 911,  82]))\n",
      "('ck', 'text', tensor([846,  58, 911,  82]))\n",
      "(' corporation', 'text', tensor([201,  56, 290,  82]))\n",
      "(' securities', 'text', tensor([125,  56, 198,  82]))\n",
      "(' s', 'text', tensor([ 93,  52, 122,  85]))\n",
      "('si', 'text', tensor([ 93,  52, 122,  85]))\n",
      "('</s>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n",
      "('<pad>', 'SPECIAL', tensor([0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "ls_token = [processor.tokenizer.decode(input_id) for input_id in encoding['input_ids']]\n",
    "ls_label = [id2label[int(label_id)] if label_id != -100 else 'SPECIAL' for label_id in encoding['labels'] ]\n",
    "ls_bb = list(encoding['bbox'])\n",
    "for item in zip(ls_token, ls_label, ls_bb):\n",
    "  print(item)\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2606efa9-5cfb-419d-afce-b7ea5eb5c247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 512])\n",
      "attention_mask torch.Size([4, 512])\n",
      "bbox torch.Size([4, 512, 4])\n",
      "labels torch.Size([4, 512])\n",
      "pixel_values torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "for item in train_dataloader:\n",
    "  for k, v in item.items():\n",
    "    print(k, v.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b6471-d0c8-4f2d-9334-5805f07b345e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6abc5a6-379c-4bce-9fe5-8c8ed2fdff28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification, AdamW\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained('microsoft/layoutlmv3-base',\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c1f3b-677b-4159-9939-c503ba2c3291",
   "metadata": {},
   "source": [
    "# Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9481cef0-a46a-4ecc-aefc-79add05c9f79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marker_phone',\n",
       " 'company_address',\n",
       " 'represented_name',\n",
       " 'marker_tax',\n",
       " 'bank_name',\n",
       " 'tax',\n",
       " 'marker_swift_code',\n",
       " 'text',\n",
       " 'marker_bank_name',\n",
       " 'marker_bank_address',\n",
       " 'bank_address',\n",
       " 'marker_company_name',\n",
       " 'fax',\n",
       " 'swift_code',\n",
       " 'account_number',\n",
       " 'marker_represented_name',\n",
       " 'marker_company_address',\n",
       " 'marker_account_number',\n",
       " 'company_name',\n",
       " 'marker_fax',\n",
       " 'phone']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60fabfcb-027c-4d75-bb8a-38863393dbf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "return_entity_level_metrics = False\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0f982d2-4644-4019-ba6e-727287173ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"layoutlmv3-fake-nonmasked-24032023\",\n",
    "                                  num_train_epochs=50,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_strategy='steps',\n",
    "                                  eval_steps=250,\n",
    "                                  save_steps=500,\n",
    "                                  save_total_limit=5,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1\",\n",
    "                                  warmup_ratio = 0.1,\n",
    "                                  do_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7e31e85-72a2-4bb5-bde7-66bae03658e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "  def get_train_dataloader(self):\n",
    "    return train_dataloader\n",
    "\n",
    "  def get_eval_dataloader(self, eval_dataset = None):\n",
    "    return val_dataloader\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705adda3-0d05-4977-b7dc-5398879f682f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tungtx2/env_ocr/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/data/tungtx2/env_ocr/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='242' max='9750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 242/9750 02:05 < 1:22:50, 1.91 it/s, Epoch 1.24/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913e7db-92bd-4b25-81e1-af18b5de73fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
