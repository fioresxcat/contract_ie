{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a01a16-c814-4ff3-b0e5-4a8d960234ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tungtx2/env_ocr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data/tungtx2/tmp/transformers_hub'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from os import listdir\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import unidecode\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pdb\n",
    "import xml.etree.ElementTree as ET\n",
    "from shapely.geometry import Polygon\n",
    "import cv2\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b95073-abbb-410e-980e-9f8993471330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_bbox(bbox, width, height):\n",
    "     return [\n",
    "         int(1000 * (bbox[0] / width)),\n",
    "         int(1000 * (bbox[1] / height)),\n",
    "         int(1000 * (bbox[2] / width)),\n",
    "         int(1000 * (bbox[3] / height)),\n",
    "     ]\n",
    "    \n",
    "    \n",
    "def parse_xml(xml_path):\n",
    "    root = ET.parse(xml_path).getroot()\n",
    "    objs = root.findall('object')\n",
    "    boxes, obj_names = [], []\n",
    "    for obj in objs:\n",
    "        obj_name = obj.find('name').text\n",
    "        box = obj.find('bndbox')\n",
    "        xmin = int(float(box.find('xmin').text))\n",
    "        ymin = int(float(box.find('ymin').text))\n",
    "        xmax = int(float(box.find('xmax').text))\n",
    "        ymax = int(float(box.find('ymax').text))\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        obj_names.append(obj_name)\n",
    "    return boxes, obj_names\n",
    "\n",
    "\n",
    "def widen_box(box, percent_x, percent_y):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "        xmin -= w * percent_x\n",
    "        ymin -= h * percent_y\n",
    "        xmax += w * percent_x\n",
    "        ymax += h * percent_y\n",
    "        return (int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "\n",
    "    \n",
    "def draw_json_on_img(img, json_data):\n",
    "    labels = list(set(shape['label'] for shape in json_data['shapes']))\n",
    "    color = {}\n",
    "    for i in range(len(labels)):\n",
    "        color[labels[i]] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "        \n",
    "    img = img.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_size = 0.5# Draw the text on the image\n",
    "    # font = ImageFont.truetype(font.font.family, font_size)\n",
    "    for i, shape in enumerate(json_data['shapes']):\n",
    "        polys = shape['points']\n",
    "        polys = [(int(pt[0]), int(pt[1])) for pt in polys]\n",
    "        label = shape['label']\n",
    "        draw.polygon(polys, outline=color[label], width=2)\n",
    "        # Draw the text on the image\n",
    "        img = np.array(img)\n",
    "        cv2.putText(img, shape['label'], (polys[0][0], polys[0][1]-5), font, font_size, color[label], thickness=1)\n",
    "        img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "    return img\n",
    "    \n",
    "    \n",
    "def mask_image(img, boxes, json_data, widen_range_x, widen_range_y):\n",
    "    # widen block\n",
    "    if isinstance(widen_range_x, list) and isinstance(widen_range_y, list):\n",
    "        boxes = [widen_box(box, np.random.uniform(widen_range_x[0], widen_range_x[1]), np.random.uniform(widen_range_y[0], widen_range_y[1])) for box in boxes]\n",
    "    else:\n",
    "        boxes = [widen_box(box, widen_range_x, widen_range_y) for box in boxes]\n",
    "        \n",
    "    \n",
    "    ls_polys2keep = []\n",
    "    ls_area2keep = []\n",
    "    iou_threshold = 0.\n",
    "    for box_idx, box in enumerate(boxes):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        box_pts = [(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)]\n",
    "        p_box = Polygon(box_pts)\n",
    "        for shape_idx, shape in enumerate(json_data['shapes']):\n",
    "            if shape_idx in ls_polys2keep:\n",
    "                continue\n",
    "            pts = shape['points']\n",
    "            p_shape = Polygon(pts)\n",
    "            intersect_area = p_box.intersection(p_shape).area\n",
    "            if intersect_area / p_shape.area > iou_threshold:\n",
    "                ls_polys2keep.append(shape_idx)\n",
    "                pts = [coord for pt in pts for coord in pt]\n",
    "                poly_xmin = min(pts[::2])\n",
    "                poly_ymin = min(pts[1::2])\n",
    "                poly_xmax = max(pts[::2])\n",
    "                poly_ymax = max(pts[1::2])\n",
    "                ls_area2keep.append((poly_xmin, poly_ymin, poly_xmax, poly_ymax))\n",
    "\n",
    "    # mask white all area of image that is not in block\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        xmin = max(0, xmin)\n",
    "        ymin = max(0, ymin)\n",
    "        xmax = min(img.shape[1], xmax)\n",
    "        ymax = min(img.shape[0], ymax)\n",
    "        mask[ymin:ymax, xmin:xmax] = 255\n",
    "\n",
    "    for area2keep in ls_area2keep:\n",
    "        xmin, ymin, xmax, ymax = area2keep\n",
    "        xmin = int(max(0, xmin))\n",
    "        ymin = int(max(0, ymin))\n",
    "        xmax = int(min(img.shape[1], xmax))\n",
    "        ymax = int(min(img.shape[0], ymax))\n",
    "        mask[ymin:ymax, xmin:xmax] = 255\n",
    "\n",
    "    # mask white\n",
    "    img[mask == 0] = 255\n",
    "\n",
    "    # delete all poly that is not in block\n",
    "    ls_idx2del = [idx for idx, shape in enumerate(json_data['shapes']) if idx not in ls_polys2keep]\n",
    "    for idx in sorted(ls_idx2del, reverse=True):\n",
    "        del json_data['shapes'][idx]\n",
    "\n",
    "    return img, json_data\n",
    "\n",
    "\n",
    "def gen_annotation_for_img(img_fp, xml_fp, json_fp, mask_type='unified', widen_range_x=[0.1, 0.2], widen_range_y=[0.1, 0.25], disable_marker=False, remove_accent=True, augment=False):\n",
    "    img = Image.open(img_fp).convert(\"RGB\")\n",
    "    json_data = json.load(open(json_fp))\n",
    "    \n",
    "    is_masked = False\n",
    "    if mask_type == 'masked' or (mask_type=='unified' and np.random.rand() < 0.5):\n",
    "        block_boxes, obj_names = parse_xml(xml_fp)\n",
    "        img, json_data = mask_image(np.array(img), boxes=block_boxes, json_data=json_data, widen_range_x=widen_range_x, widen_range_y=widen_range_y)\n",
    "        img = Image.fromarray(img)\n",
    "        is_masked = True\n",
    "    \n",
    "    if augment and np.random.rand() < 0.3:  # random drop some boxes\n",
    "        size = int(0.08*len(json_data['shapes'])) if not is_masked else int(0.05*len(json_data['shapes']))\n",
    "        idx2drop = np.random.choice(list(range(len(json_data['shapes']))), size=size)\n",
    "        json_data['shapes'] = [shape for i, shape in enumerate(json_data['shapes']) if i not in idx2drop]\n",
    "            \n",
    "    # pdb.set_trace()\n",
    "        \n",
    "    words, orig_polys, normalized_boxes, labels = [], [], [], []\n",
    "    img_h, img_w = json_data['imageHeight'], json_data['imageWidth']\n",
    "    for i, shape in enumerate(json_data['shapes']):\n",
    "        if disable_marker and 'marker' in shape['label']:\n",
    "            current_label = 'text'\n",
    "        else:\n",
    "            current_label = shape['label']\n",
    "        \n",
    "        if remove_accent:\n",
    "            words.append(unidecode.unidecode(shape['text'].lower()))\n",
    "        else:\n",
    "            words.append(shape['text'].lower())\n",
    "            \n",
    "        labels.append(current_label)\n",
    "        pts = [coord for pt in shape['points'] for coord in pt]\n",
    "        xmin = min(pts[0::2])\n",
    "        xmax = max(pts[0::2])\n",
    "        ymin = min(pts[1::2])\n",
    "        ymax = max(pts[1::2])\n",
    "\n",
    "        xmin = max(xmin, 0)\n",
    "        ymin = max(ymin, 0)\n",
    "        xmax = min(img_w, xmax)\n",
    "        ymax = min(img_h, ymax)\n",
    "\n",
    "        normalized_boxes.append(normalize_bbox((xmin, ymin, xmax, ymax), img_w, img_h))\n",
    "        orig_polys.append(tuple([tuple(pt) for pt in shape['points']]))\n",
    "    \n",
    "    return img, words, orig_polys, normalized_boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a2715-4eaa-424e-a6a8-2512952d45aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import LayoutLMv3PreTrainedModel\n",
    "\n",
    "class LayoutLMv3ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Head for sentence-level classification tasks. Reference: RobertaClassificationHead\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(in_features=config.hidden_size, out_features=config.hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.1, inplace=False)\n",
    "        self.out_proj =  nn.Linear(in_features=config.hidden_size, out_features=num_labels, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MyTransformerForTokenClassification(LayoutLMv3PreTrainedModel):\n",
    "    def __init__(self, backbone, dropout_prob, num_groups, num_labels):\n",
    "        super().__init__(backbone.config)\n",
    "        self.backbone = backbone\n",
    "        self.config = self.backbone.config\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.group_classifier = nn.Linear(self.config.hidden_size, num_groups)\n",
    "        self.ner_classifier = LayoutLMv3ClassificationHead(self.config, num_labels)\n",
    "\n",
    "            \n",
    "    def forward(self, pixel_values, input_ids, bbox, attention_mask):\n",
    "        backbone_out = self.backbone(pixel_values=pixel_values, input_ids=input_ids, bbox=bbox, attention_mask=attention_mask)\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()  # (batch, sequence length)\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        # only take the text part of the output representations\n",
    "        sequence_output = backbone_out[0][:, :seq_length]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        ner_logits = self.ner_classifier(sequence_output)\n",
    "        group_logits = self.group_classifier(sequence_output)\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "#         if not return_dict:\n",
    "#             output = (logits,) + outputs[1:]\n",
    "#             return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "#         return TokenClassifierOutput(\n",
    "#             loss=loss,\n",
    "#             logits=logits,\n",
    "#             hidden_states=outputs.hidden_states,\n",
    "#             attentions=outputs.attentions,\n",
    "#         )\n",
    "        return ner_logits, group_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfa0059-757d-4c6b-9248-6f371aaec25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3Processor, LayoutLMv3Model\n",
    "\n",
    "processor = LayoutLMv3Processor.from_pretrained('microsoft/layoutlmv3-base', apply_ocr=False)\n",
    "processor.tokenizer.only_label_first_subword = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bcacec-a1db-414c-810b-4d893d8dff9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'swift_code': 0, 'marker_swift_code': 1, 'bank_name': 2, 'company_name': 3, 'tax': 4, 'bank_address': 5, 'marker_bank_address': 6, 'marker_represented_name': 7, 'marker_account_number': 8, 'marker_represented_position': 9, 'marker_fax': 10, 'marker_phone': 11, 'marker_company_address': 12, 'text': 13, 'marker_bank_name': 14, 'account_number': 15, 'company_address': 16, 'fax': 17, 'marker_tax': 18, 'marker_company_name': 19, 'represented_position': 20, 'phone': 21, 'represented_name': 22}\n",
    "type(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372b133d-e65c-4846-87fe-2aed9a439bee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 20, 20, 22, 22, 22, 7, 13, 22, 7, 17, 17, 10, 21, 21, 11, 16, 16, 16, 16, 3, 3, 3, 3, 19, 19, 20, 20, 13, 22, 22, 22, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 15, 8, 8, 14, 17, 10, 17, 17, 17, 21, 21, 21, 11, 4, 18, 18, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 3, 3, 3, 3, 3, 3, 3, 19, 19, 19, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0,  5],\n",
       "         [ 5,  8],\n",
       "         ...,\n",
       "         [ 0,  8],\n",
       "         [ 0, 11],\n",
       "         [ 0,  0]],\n",
       "\n",
       "        [[ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 1,  3],\n",
       "         ...,\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_fp = Path('real_data/val_labeled_ocred/CTR292 (1)-001_0.jpg')\n",
    "img, words, orig_polys, normalized_boxes, labels = gen_annotation_for_img(img_fp=img_fp, \n",
    "                                                                          xml_fp=img_fp.with_suffix('.xml'),\n",
    "                                                                         json_fp=img_fp.with_suffix('.json'),\n",
    "                                                                         mask_type='unmasked')\n",
    "idx_labels = [label2id[label] for label in labels]\n",
    "print(idx_labels)\n",
    "# encode input for model\n",
    "encoded_inputs = processor(img, words, boxes=normalized_boxes, word_labels=idx_labels, truncation=True, stride=128,\n",
    "                           padding=\"max_length\", max_length=512, return_overflowing_tokens=True, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "encoded_inputs.pop('overflow_to_sample_mapping')\n",
    "encoded_inputs.pop('offset_mapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73d5893-f3ae-4634-97e5-1cd32a95cd00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = encoded_inputs['input_ids'].to(device)\n",
    "bbox = encoded_inputs['bbox'].to(device)\n",
    "attention_mask = encoded_inputs['attention_mask'].to(device)\n",
    "pixel_values = torch.stack(encoded_inputs['pixel_values'], dim=0).to(device)\n",
    "labels = encoded_inputs['labels'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdcd44c2-3250-4b16-9ec0-dbef7a265e21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d92254-fa92-4cbb-bf8d-52eb454b4f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = LayoutLMv3Model.from_pretrained('microsoft/layoutlmv3-base')\n",
    "dropout_prob = 0.1\n",
    "num_groups = 4\n",
    "num_labels = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974329bd-884e-4c53-9257-27748a7652f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MyTransformerForTokenClassification(backbone, dropout_prob, num_groups, num_labels).eval().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8a6f0-c89f-4a4b-ae81-bd97e29f5038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d6b5f-8407-4629-b210-36d5a0fcd383",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ner_out, group_out = model(pixel_values=pixel_values, input_ids=input_ids, bbox=bbox, attention_mask=attention_mask)\n",
    "print(ner_out.shape)\n",
    "print(group_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad12c5-b86e-465d-afe3-1eaceb1cdc7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir(model.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cecfb-c757-4d6c-968c-4c9e3f122d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c6a2cf-bc86-495b-9bd3-fb2f93133083",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForTokenClassification(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=23, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "\n",
    "lmv3_model = LayoutLMv3ForTokenClassification.from_pretrained('microsoft/layoutlmv3-base', num_labels=23).to(device)\n",
    "lmv3_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba10588-1c6c-4b98-a6af-207eab184020",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tungtx2/env_ocr/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1024) to match target batch_size (46).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7089/2527550400.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                         labels=ner_label)\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m# print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tungtx2/env_ocr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tungtx2/env_ocr/lib/python3.7/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, pixel_values)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tungtx2/env_ocr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tungtx2/env_ocr/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tungtx2/env_ocr/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1024) to match target batch_size (46)."
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = torch.optim.AdamW(lmv3_model.parameters(), lr=1e-3)\n",
    "criterion_ner = nn.CrossEntropyLoss()\n",
    "criterion_group = nn.CrossEntropyLoss()\n",
    "\n",
    "ner_label = torch.arange(46).reshape((2, 23)).to(device)\n",
    "group_label = torch.arange(8).reshape((2, 4)).to(device)\n",
    "num_train_epochs = 150\n",
    "\n",
    "# model.backbone.train()\n",
    "# model.train()\n",
    "\n",
    "# # put the model in training mode\n",
    "# for epoch in range(num_train_epochs):\n",
    "#     # zero the parameter gradients\n",
    "#     optimizer.zero_grad()\n",
    "#     # forward + backward + optimize\n",
    "#     ner_out, group_out = lmv3_model(input_ids=input_ids,\n",
    "#                                 bbox=bbox,\n",
    "#                                 pixel_values=pixel_values,\n",
    "#                                 attention_mask=attention_mask)\n",
    "#     loss_ner = criterion_ner(ner_out, ner_label)\n",
    "#     loss_group = criterion_group(group_out, group_label)\n",
    "#     loss = loss_ner + loss_group\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print('loss: ', loss)\n",
    "\n",
    "# put the model in training mode\n",
    "\n",
    "lmv3_model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = lmv3_model(input_ids=input_ids,\n",
    "                                bbox=bbox,\n",
    "                                pixel_values=pixel_values,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=ner_label)\n",
    "    # print(outputs)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f875e4a-ca55-4725-b0dc-ef49a7f24a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 23])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_label.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
